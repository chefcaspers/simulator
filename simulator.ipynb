{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f72f1189-48c8-4d48-91b2-bc6f62381f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install osmnx==1.7.1 networkx==3.2.1 geopy==2.4.1 shapely==2.0.3 databricks-sdk[openai]>=0.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4f10c8-0769-4289-b921-04e82eeec549",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load config"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    sim_cfg_json = dbutils.widgets.get(\"sim_cfg_json\")\n",
    "except:\n",
    "    print(\"no widget\")\n",
    "    sim_cfg_json = ''\n",
    "    \n",
    "SIM_CFG = json.loads(sim_cfg_json) if sim_cfg_json != '' else json.load(open('./config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea2193d-2881-447b-bfe2-52051227d2cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "data bootstrap"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ghost-kitchen menu generator\n",
    "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "â€¢ Creates four relational datasets: brands, menus, categories, items\n",
    "â€¢ Adds deterministic, â€œreasonableâ€ USD prices to each item\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1. SOURCE DATA (renamed real-world menus) ----------\n",
    "brands_data = [\n",
    "    (\"McDoodles\", {\n",
    "        \"Burgers\": [\"Big Stack\", \"Quarter Poundish\", \"Double Quarter Poundish\", \"Cheese Burgerette\"],\n",
    "        \"Chicken & Fish Sandwiches\": [\"McCrisper\", \"Spicy McCrisper\", \"Filet-O-Fishish\", \"McChicklet\"],\n",
    "        \"Nuggets\": [\"Chick Nuggies 6pc\"],\n",
    "        \"Fries & Sides\": [\"Famous Frites\", \"Apple Slice Bites\"],\n",
    "        \"Breakfast\": [\"Egg McMuffle\", \"Bacon Biscuit\"],\n",
    "        \"Coffees & Beverages\": [\"Iced Brew\", \"Fizzy Cola\"]\n",
    "    }),\n",
    "    (\"ChipoLot\", {\n",
    "        \"Burritos\": [\"Chicken Burrito\", \"Steak Burrito\", \"Barbacoa Burrito\", \"Veggie Burrito\"],\n",
    "        \"Bowls\": [\"Chicken Bowl\", \"Keto Bowl\", \"High Protein Bowl\"],\n",
    "        \"Tacos\": [\"Chicken Tacos\", \"Carnitas Tacos\"],\n",
    "        \"Salads\": [\"Supergreens Salad\"],\n",
    "        \"Kids\": [\"Build-Your-Own Kid Meal\"],\n",
    "        \"Sides & Extras\": [\"Chips & Guac\", \"Chips & Queso\"]\n",
    "    }),\n",
    "    (\"Shack Stack\", {\n",
    "        \"Burgers\": [\"ShackBurger Single\", \"ShackBurger Double\", \"Smoke Stack Single\", \"Veggie Stack\"],\n",
    "        \"Chicken\": [\"Chicken Shack\", \"Avocado Bacon Chicken\"],\n",
    "        \"Dogs\": [\"Flat-Top Dog\"],\n",
    "        \"Fries\": [\"Crinkle Fries\", \"Cheese Fries\"],\n",
    "        \"Shakes\": [\"Vanilla Shake\", \"Chocolate Shake\"],\n",
    "        \"Drinks\": [\"Shack-made Lemonade\"]\n",
    "    }),\n",
    "    (\"GreenSprout\", {\n",
    "        \"Bowls\": [\"Harvest Bowl\", \"Crispy Rice Bowl\", \"Shroomami\"],\n",
    "        \"Salads\": [\"Kale Caesar\", \"Guac Greens\", \"Buffalo Chicken Salad\"],\n",
    "        \"Protein Plates\": [\"Garlic Steak Plate\", \"Hot Honey Chicken Plate\"],\n",
    "        \"Kids\": [\"Little Harvest\"],\n",
    "        \"Sides\": [\"Rosemary Focaccia\", \"Hummus & Focaccia\"],\n",
    "        \"Drinks\": [\"Jasmine Green Tea\"]\n",
    "    }),\n",
    "    (\"Pando Dash\", {\n",
    "        \"Entrees\": [\"Orange Chickadee\", \"Kung Pow Chicken\", \"Beijing Beefy\", \"Honey Walnut Shrimpish\"],\n",
    "        \"Sides\": [\"Chow Mein Noodles\", \"Fried Rice\"],\n",
    "        \"Appetizers\": [\"Cream Cheese Rangoon\", \"Chicken Egg Roll\"],\n",
    "        \"Drinks\": [\"Fountain Soda\"],\n",
    "        \"Bundles\": [\"Bowl Combo\"]\n",
    "    }),\n",
    "    (\"Kava Kitchen\", {\n",
    "        \"Bowls\": [\"Spicy Lamb & Avo Bowl\", \"Falafel Crunch Bowl\", \"Chicken + Rice Bowl\"],\n",
    "        \"Pitas\": [\"Steak & Feta Pita\", \"Crispy Falafel Pita\"],\n",
    "        \"Kids\": [\"Mini Pita Meal\"],\n",
    "        \"Drinks\": [\"Strawberry Citrus Lemonade\"],\n",
    "        \"Sides\": [\"Hot Harissa Chips\"],\n",
    "        \"Desserts\": [\"Salted Choco Cookie\"]\n",
    "    }),\n",
    "    (\"MaruNoodle\", {\n",
    "        \"Udon\": [\"Beef Udon\", \"Chicken Katsu Udon\", \"Curry Udon\"],\n",
    "        \"Rice Bowls\": [\"Gyudon Rice\", \"Yakitori Rice\"],\n",
    "        \"Tempura\": [\"Shrimp Tempura\", \"Sweet Potato Tempura\"],\n",
    "        \"Onigiri\": [\"Spam Musubi Onigiri\"],\n",
    "        \"Add-Ons\": [\"BK Sauce\"]\n",
    "    }),\n",
    "    (\"Jinya Noodle Bar\", {\n",
    "        \"Ramen\": [\"Tonkotsu Ramen\", \"Spicy Chicken Ramen\", \"Vegan Ramen\"],\n",
    "        \"Small Plates\": [\"Pork Gyoza\", \"Takoyaki\"],\n",
    "        \"Rice Bowls\": [\"Chicken Karaage Rice Bowl\"],\n",
    "        \"Toppings\": [\"Extra Chashu\", \"Spicy Sauce\"],\n",
    "        \"Drinks\": [\"Green Tea\"]\n",
    "    }),\n",
    "    (\"Fo Ho\", {\n",
    "        \"Pho\": [\"Pho Combo\", \"Pho Chicken\"],\n",
    "        \"Rolls\": [\"Fried Spring Roll\", \"Summer Roll\"],\n",
    "        \"Rice Plates\": [\"Grilled Pork Rice Plate\"],\n",
    "        \"Vermicelli\": [\"Lemongrass Chicken Vermicelli\"],\n",
    "        \"Drinks\": [\"Vietnamese Iced Coffee\"]\n",
    "    }),\n",
    "    (\"Yo! Sushii\", {\n",
    "        \"Maki\": [\"California Roll\", \"Spicy Tuna Roll\"],\n",
    "        \"Nigiri\": [\"Salmon Nigiri\", \"Ebi Nigiri\"],\n",
    "        \"Sashimi\": [\"Tuna Sashimi\"],\n",
    "        \"Street Food\": [\"Chicken Katsu Curry\", \"Yakisoba\", \"Miso Soup\"],\n",
    "        \"Desserts\": [\"Mochi Ice Cream\"],\n",
    "        \"Drinks\": [\"Matcha Soda\"]\n",
    "    }),\n",
    "    (\"Taco Ring\", {\n",
    "        \"Tacos\": [\"Crunchy Taco\", \"Soft Taco Supreme\"],\n",
    "        \"Burritos\": [\"Beefy 5-Layer Burrito\"],\n",
    "        \"Specialties\": [\"Doritos Locos Taco\"],\n",
    "        \"Quesadillas\": [\"Chicken Quesadilla\"],\n",
    "        \"Nachos\": [\"Nacho Bellgrande\"],\n",
    "        \"Sides\": [\"Cinnamon Twists\"],\n",
    "        \"Drinks\": [\"Baja Blast Soda\"]\n",
    "    }),\n",
    "    (\"Starbrews\", {\n",
    "        \"Hot Coffee\": [\"CaffÃ¨ Latte\", \"Pike Place Brew\"],\n",
    "        \"Cold Coffee\": [\"Cold Brew\"],\n",
    "        \"Refreshers\": [\"Strawberry AÃ§aÃ­ Refresher\"],\n",
    "        \"Frappuccinos\": [\"Caramel Frappuccino\"],\n",
    "        \"Bakery\": [\"Plain Bagel\"],\n",
    "        \"Breakfast Sandwiches\": [\"Sausage Cheddar Egg Sandwich\"],\n",
    "        \"Tea\": [\"Green Tea\"]\n",
    "    }),\n",
    "    (\"Dominni Pizza\", {\n",
    "        \"Specialty Pizzas\": [\"ExtravaganZZza Pizza\", \"MeatZZa Pizza\", \"Pacific Veggie Pizza\"],\n",
    "        \"Build Your Own Pizza\": [\"Custom Pizza\"],\n",
    "        \"Pasta\": [\"Chicken Alfredo Pasta\"],\n",
    "        \"Chicken\": [\"Boneless Chicken\"],\n",
    "        \"Sides\": [\"Bread Twists\"],\n",
    "        \"Desserts\": [\"Chocolate Lava Cake\"],\n",
    "        \"Drinks\": [\"Coke\"]\n",
    "    }),\n",
    "    (\"Five Gals\", {\n",
    "        \"Burgers\": [\"Bacon Cheeseburger\", \"Little Cheeseburger\"],\n",
    "        \"Dogs\": [\"Kosher Style Hot Dog\"],\n",
    "        \"Sandwiches\": [\"BLT Sandwich\"],\n",
    "        \"Fries\": [\"Cajun Fries\"],\n",
    "        \"Milkshakes\": [\"Vanilla Milkshake\"],\n",
    "        \"Drinks\": [\"Bottled Soda\"]\n",
    "    }),\n",
    "    (\"PokeCraft\", {\n",
    "        \"Poke Bowls\": [\"Signature Hawaiian Bowl\", \"Spicy Ahi Bowl\"],\n",
    "        \"Burritos\": [\"Chicken Katsu Burrito\"],\n",
    "        \"Salads\": [\"Vegan Salad\"],\n",
    "        \"Sides\": [\"Miso Soup Side\"],\n",
    "        \"Drinks\": [\"Japanese Soda\"]\n",
    "    })\n",
    "]\n",
    "\n",
    "# ---------- 2. PRICE BANDS BY CATEGORY KEYWORD ----------\n",
    "pricing_rules = [\n",
    "    ([\"pizza\"], (12, 18)),\n",
    "    ([\"burger\"], (6, 11)),\n",
    "    ([\"burrito\"], (7, 10)),\n",
    "    ([\"bowl\"], (9, 12)),\n",
    "    ([\"udon\", \"ramen\"], (10, 14)),\n",
    "    ([\"salad\"], (8, 11)),\n",
    "    ([\"pho\", \"rice plate\", \"plate\"], (9, 13)),\n",
    "    ([\"kids\"], (4, 6)),\n",
    "    ([\"fries\"], (2, 4)),\n",
    "    ([\"side\", \"sides\"], (2, 5)),\n",
    "    ([\"drink\", \"coffee\", \"tea\", \"soda\", \"lemonade\"], (2, 4)),\n",
    "    ([\"shake\", \"frappuccino\"], (4, 6)),\n",
    "    ([\"dessert\", \"mochi\", \"cookie\", \"cake\"], (3, 5)),\n",
    "    ([\"sandwich\"], (4, 7)),\n",
    "    ([\"dog\"], (3, 6)),\n",
    "    ([\"nugget\"], (3, 5)),\n",
    "    ([\"taco\"], (2, 4)),\n",
    "    ([\"quesa\", \"nacho\"], (3, 6)),\n",
    "    ([\"roll\", \"nigiri\"], (4, 7)),\n",
    "    ([\"sashimi\"], (5, 9)),\n",
    "    ([\"tempura\"], (4, 7)),\n",
    "    ([\"onigiri\"], (3, 5)),\n",
    "    ([\"rice\"], (8, 11)),\n",
    "]\n",
    "\n",
    "def deterministic_random(item_name: str, low: int, high: int) -> float:\n",
    "    \"\"\"Hash item name to a deterministic price within [low, high).\"\"\"\n",
    "    h = hashlib.md5(item_name.encode()).hexdigest()\n",
    "    cents_range = (high - low) * 100\n",
    "    return round(low + (int(h[:8], 16) % cents_range) / 100, 2)\n",
    "\n",
    "def get_price(item_name: str, category_name: str) -> float:\n",
    "    lower_cat = category_name.lower()\n",
    "    for keywords, (lo, hi) in pricing_rules:\n",
    "        if any(k in lower_cat for k in keywords):\n",
    "            return deterministic_random(item_name, lo, hi)\n",
    "    return deterministic_random(item_name, 6, 10)  # fallback\n",
    "\n",
    "# ---------- 3. BUILD TABLE ROWS ----------\n",
    "brands_rows, menus_rows, categories_rows, items_rows = [], [], [], []\n",
    "\n",
    "brand_id = menu_id = category_id = item_id = 1\n",
    "for brand, menu_struct in brands_data:\n",
    "    brands_rows.append({\"id\": brand_id, \"name\": brand})\n",
    "    menus_rows.append({\"id\": menu_id, \"brand_id\": brand_id, \"name\": f\"{brand} Main Menu\"})\n",
    "    for cat, items in menu_struct.items():\n",
    "        categories_rows.append({\"id\": category_id, \"menu_id\": menu_id, \"brand_id\": brand_id,\"name\": cat})\n",
    "        for it in items:\n",
    "            price = get_price(it, cat)\n",
    "            items_rows.append({\n",
    "                \"id\": item_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"menu_id\": menu_id,\n",
    "                \"brand_id\": brand_id,\n",
    "                \"name\": it,\n",
    "                \"price\": price\n",
    "            })\n",
    "            item_id += 1\n",
    "        category_id += 1\n",
    "    brand_id += 1\n",
    "    menu_id += 1\n",
    "\n",
    "# ---------- 4. TO DATAFRAMES ----------\n",
    "brands_df     = pd.DataFrame(brands_rows)\n",
    "menus_df      = pd.DataFrame(menus_rows)\n",
    "categories_df = pd.DataFrame(categories_rows)\n",
    "items_df      = pd.DataFrame(items_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c19b9f7-e685-4913-91a5-7cb9c7b00ed5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "catalog entires"
    }
   },
   "outputs": [],
   "source": [
    "catalog = SIM_CFG['catalog']\n",
    "schema = SIM_CFG['schema']\n",
    "table = SIM_CFG['table']\n",
    "\n",
    "spark.sql(f\"DROP SCHEMA IF EXISTS {catalog}.{schema} CASCADE\")\n",
    "spark.sql(f\"CREATE SCHEMA {catalog}.{schema}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.brand (\n",
    "  id   BIGINT NOT NULL,\n",
    "  name STRING NOT NULL,\n",
    "  CONSTRAINT brand_pk PRIMARY KEY (id)\n",
    ")\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.menu (\n",
    "  id       BIGINT NOT NULL,\n",
    "  name     STRING NOT NULL,\n",
    "  brand_id BIGINT NOT NULL,\n",
    "\n",
    "  CONSTRAINT menu_pk        PRIMARY KEY (id),\n",
    "  CONSTRAINT menu_brand_fk  FOREIGN KEY (brand_id)\n",
    "    REFERENCES {catalog}.{schema}.brand(id)\n",
    ")\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.category (\n",
    "  id       BIGINT NOT NULL,\n",
    "  name     STRING NOT NULL,\n",
    "  menu_id  BIGINT NOT NULL,\n",
    "  brand_id BIGINT NOT NULL,\n",
    "\n",
    "  CONSTRAINT category_pk         PRIMARY KEY (id),\n",
    "  CONSTRAINT category_menu_fk    FOREIGN KEY (menu_id)\n",
    "      REFERENCES {catalog}.{schema}.menu(id),\n",
    "  CONSTRAINT category_brand_fk   FOREIGN KEY (brand_id)\n",
    "      REFERENCES {catalog}.{schema}.brand(id)\n",
    ")\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{schema}.item (\n",
    "  id          BIGINT NOT NULL,\n",
    "  name        STRING NOT NULL,\n",
    "  description STRING,\n",
    "  price       DOUBLE NOT NULL,\n",
    "  image_data  STRING,\n",
    "\n",
    "  brand_id    BIGINT NOT NULL,\n",
    "  menu_id     BIGINT NOT NULL,\n",
    "  category_id BIGINT NOT NULL,\n",
    "\n",
    "  CONSTRAINT item_pk            PRIMARY KEY (id),\n",
    "  CONSTRAINT item_brand_fk      FOREIGN KEY (brand_id)\n",
    "      REFERENCES {catalog}.{schema}.brand(id),\n",
    "  CONSTRAINT item_menu_fk       FOREIGN KEY (menu_id)\n",
    "      REFERENCES {catalog}.{schema}.menu(id),\n",
    "  CONSTRAINT item_category_fk   FOREIGN KEY (category_id)\n",
    "      REFERENCES {catalog}.{schema}.category(id)\n",
    ")\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {catalog}.{schema}.{table} (\n",
    "  event_id   STRING,\n",
    "  event_type STRING,\n",
    "  ts         TIMESTAMP,\n",
    "  gk_id      STRING,\n",
    "  order_id   STRING,\n",
    "  sequence   INT,\n",
    "  body       STRING,\n",
    "  day        DATE\n",
    ")\n",
    "USING delta\n",
    "PARTITIONED BY (day)\n",
    "TBLPROPERTIES(delta.minReaderVersion=2, delta.minWriterVersion=5)\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE VOLUME {catalog}.{schema}.cache\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eeee9de-0b1e-47e0-b941-d4c054f1c95f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write data to catalog"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(brands_df).write.mode(\"overwrite\").saveAsTable(\"brand\")\n",
    "spark.createDataFrame(menus_df).write.mode(\"overwrite\").saveAsTable(\"menu\")\n",
    "spark.createDataFrame(categories_df).write.mode(\"overwrite\").saveAsTable(\"category\")\n",
    "spark.createDataFrame(items_df).write.mode(\"overwrite\").saveAsTable(\"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000b80e0-b045-41b1-b55b-6f0b6d45501a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Ghost-Kitchen Event Simulator 2.1  â†’  Delta table                    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#\n",
    "#  All tunables live in SIM_CFG  â€“ supply as a JSON widget or edit below\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "import asyncio, datetime as dt, json, math, pickle, random, time, uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nest_asyncio, networkx as nx, numpy as np, osmnx as ox, pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "nest_asyncio.apply()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0.  CONFIGURATION                                                      â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CFG = SIM_CFG; get = lambda k: CFG[k]\n",
    "RAND = random.Random(get(\"random_seed\")); np.random.seed(get(\"random_seed\"))\n",
    "\n",
    "CATALOG, SCHEMA, TABLE = get(\"catalog\"), get(\"schema\"), get(\"table\")\n",
    "TABLE_FQN              = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "START_TS, END_TS       = map(lambda t: dt.datetime.strptime(t,\"%Y-%m-%d %H:%M:%S\"),\n",
    "                             (get(\"start_ts\"), get(\"end_ts\")))\n",
    "SPEED_UP               = get(\"speed_up\")\n",
    "GK_ADDRESS, GK_R_MI    = get(\"gk_location\"), get(\"radius_mi\")\n",
    "GK_DRIVER_MPH          = get(\"driver_mph\")\n",
    "NOISE, SVC             = get(\"noise_pct\")/100, get(\"svc\")\n",
    "BATCH_ROWS, BATCH_SEC  = get(\"batch_rows\"), get(\"batch_seconds\")\n",
    "PING_SEC               = get(\"ping_sec\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1.  ROAD GRAPH + NODES (cached)                                        â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CACHE_DIR = f\"/Volumes/{CATALOG}/{SCHEMA}/cache\"\n",
    "GRAPH_PKL, NODES_PARQ = Path(CACHE_DIR, \"roadGraph.pkl\"), Path(CACHE_DIR, \"nodes.parquet\")\n",
    "Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_graph() -> nx.MultiDiGraph:\n",
    "    if GRAPH_PKL.exists(): return pickle.loads(GRAPH_PKL.read_bytes())\n",
    "    ox.settings.log_console = False\n",
    "    g = ox.graph_from_point(ox.geocoder.geocode(GK_ADDRESS),\n",
    "                            dist=GK_R_MI*1609.34, network_type=\"drive\")\n",
    "    GRAPH_PKL.write_bytes(pickle.dumps(g)); return g\n",
    "\n",
    "def load_nodes(g) -> pd.DataFrame:\n",
    "    if NODES_PARQ.exists(): return pd.read_parquet(NODES_PARQ)\n",
    "    rows=[]\n",
    "    for nid,data in g.nodes(data=True):\n",
    "        h,s = data.get(\"addr:housenumber\"), data.get(\"addr:street\")\n",
    "        label = f\"{h} {s}\" if h and s else f\"{RAND.randint(1,9999)} Main St\"\n",
    "        rows.append(dict(node_id=nid, lat=data[\"y\"], lon=data[\"x\"], addr=label))\n",
    "    df = pd.DataFrame(rows); df.to_parquet(NODES_PARQ, index=False); return df\n",
    "\n",
    "GRAPH = load_graph(); NODES = load_nodes(GRAPH)\n",
    "GK_LAT, GK_LON = ox.geocoder.geocode(GK_ADDRESS)\n",
    "GK_NODE = ox.distance.nearest_nodes(GRAPH, GK_LON, GK_LAT)\n",
    "comp_map = {n:cid for cid,comp in enumerate(nx.connected_components(GRAPH.to_undirected())) for n in comp}\n",
    "NODES = NODES[NODES.node_id.map(comp_map)==comp_map[GK_NODE]].reset_index(drop=True)\n",
    "def rand_customer():\n",
    "    r = NODES.sample(1, random_state=RAND.randrange(2**32)).iloc[0]  # âœ”\n",
    "    return int(r.node_id), r.lat, r.lon, r.addr\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2.  MENU + BRAND MOMENTUM                                              â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ITEMS_DF = spark.table(\"item\").toPandas()\n",
    "ITEMS_BY_BRAND = {bid: grp.to_dict(\"records\") for bid, grp in ITEMS_DF.groupby(\"brand_id\")}\n",
    "BRANDS = list(ITEMS_BY_BRAND); RAND.shuffle(BRANDS)\n",
    "\n",
    "bm = get(\"brand_momentum\"); cuts = np.cumsum([bm[\"improving\"], bm[\"flat\"]])*len(BRANDS)\n",
    "IMPR, FLAT, DECL = BRANDS[:int(cuts[0])], BRANDS[int(cuts[0]):int(cuts[1])], BRANDS[int(cuts[1]):]\n",
    "rates = get(\"momentum_rates\")\n",
    "\n",
    "def brand_weight(day, total, b):\n",
    "    if b in IMPR:  f = (1+rates[\"growth\"])**(day/30)\n",
    "    elif b in DECL: f = (1-rates[\"decline\"])**(day/30)\n",
    "    else:          f = 1.0\n",
    "    return f\n",
    "\n",
    "def rand_basket(day, total, p_single=0.7, max_brands=4, items_rng=(1,4)):\n",
    "    w = np.array([brand_weight(day,total,b) for b in BRANDS]); w = w/w.sum()\n",
    "    chosen = [RAND.choices(BRANDS, weights=w, k=1)[0]] if RAND.random()<p_single \\\n",
    "             else RAND.choices(BRANDS, weights=w, k=RAND.randint(2,max_brands))\n",
    "    items=[]\n",
    "    for b in chosen:\n",
    "        for itm in RAND.sample(ITEMS_BY_BRAND[b], RAND.randint(*items_rng)):\n",
    "            rec = itm.copy(); rec[\"qty\"] = RAND.randint(1,3); items.append(rec)\n",
    "    return items\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3.  DEMAND SHAPE                                                       â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def minute_weights():\n",
    "    w = np.ones(1440)\n",
    "    for s,e,m in [(\"11:00\",\"13:30\",3), (\"17:00\",\"20:00\",3.5)]:\n",
    "        s_dt,e_dt = [dt.datetime.strptime(x,\"%H:%M\") for x in (s,e)]\n",
    "        s_m,e_m = s_dt.hour*60+s_dt.minute, e_dt.hour*60+e_dt.minute; span=e_m-s_m\n",
    "        for mi in range(s_m,e_m):\n",
    "            x = (mi-s_m)/span; w[mi] += (m-1)*(math.sin(math.pi*x)**2)\n",
    "    return w\n",
    "MIN_W = minute_weights()\n",
    "\n",
    "def orders_today(d, total, date):\n",
    "    base = CFG[\"orders_day_1\"]+(CFG[\"orders_last\"]-CFG[\"orders_day_1\"])*(d/total)\n",
    "    wd = {\"mon\":1,\"tue\":1.05,\"wed\":1.08,\"thu\":1.10,\"fri\":1.25,\"sat\":1.35,\"sun\":1.15}[date.strftime(\"%a\").lower()]\n",
    "    return base*wd*RAND.uniform(1-NOISE,1+NOISE)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4.  ROUTING                                                            â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def shortest_route(lat,lon):\n",
    "    cust = ox.distance.nearest_nodes(GRAPH, lon, lat)\n",
    "    try:\n",
    "        path = nx.shortest_path(GRAPH, GK_NODE, cust, weight=\"length\"); g = GRAPH\n",
    "    except nx.NetworkXNoPath:\n",
    "        g = GRAPH.to_undirected(); path = nx.shortest_path(g, GK_NODE, cust, weight=\"length\")\n",
    "    coords = [(g.nodes[n][\"y\"], g.nodes[n][\"x\"]) for n in path]\n",
    "    dist = sum(min(d[\"length\"] for d in g[u][v].values()) for u,v in zip(path[:-1],path[1:]))\n",
    "    return coords, dist\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5.  DELTA WRITER + DATA-QUALITY                                        â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EVENT_Q = asyncio.PriorityQueue(); CNT = 0\n",
    "GK_ID = uuid.uuid4().hex; DQ = get(\"dq\")\n",
    "\n",
    "def maybe_corrupt(ev, payload):\n",
    "    dq = DQ.get(ev, {}); return {k:(None if RAND.random()<dq.get(k,0) else v) for k,v in payload.items()}\n",
    "\n",
    "def enqueue(ts, ev, oid, seq, payload):\n",
    "    global CNT; CNT += 1\n",
    "    EVENT_Q.put_nowait((ts.timestamp(), CNT, {\n",
    "        \"event_id\": uuid.uuid4().hex,\n",
    "        \"event_type\": ev,\n",
    "        \"ts\": ts.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
    "        \"gk_id\": GK_ID,\n",
    "        \"order_id\": oid,\n",
    "        \"sequence\": seq,\n",
    "        \"body\": json.dumps(maybe_corrupt(ev, payload))\n",
    "    }))\n",
    "\n",
    "def flush(rows):\n",
    "    if not rows: return\n",
    "    (spark.createDataFrame(rows)\n",
    "          .withColumn(\"ts\", F.to_timestamp(\"ts\",\"yyyy-MM-dd HH:mm:ss.SSSSSS\"))\n",
    "          .withColumn(\"day\",F.to_date(\"ts\"))\n",
    "          .withColumn(\"sequence\",F.col(\"sequence\").cast(\"INT\"))\n",
    "          .write.format(\"delta\").mode(\"append\").saveAsTable(TABLE_FQN))\n",
    "\n",
    "async def consumer():\n",
    "    buf,last=[],time.time()\n",
    "    while True:\n",
    "        _,_,row = await EVENT_Q.get(); buf.append(row)\n",
    "        if len(buf)>=BATCH_ROWS or (time.time()-last)>=BATCH_SEC:\n",
    "            flush(buf); buf.clear(); last=time.time()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6.  ORDER LIFECYCLE (real-time)                                        â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MICRO = lambda n: dt.timedelta(microseconds=n)\n",
    "def gauss(mu_sigma): return max(0.1, RAND.gauss(*mu_sigma))\n",
    "\n",
    "driver_cfg = get(\"driver_arrival\")\n",
    "def driver_arrival_time(created_at, t_ready, t_pick):\n",
    "    \"\"\"Return a dt between created_at and t_pick with prob mass after ready.\"\"\"\n",
    "    if RAND.random() < driver_cfg[\"after_ready_pct\"]:\n",
    "        base, span = t_ready, (t_pick - t_ready)\n",
    "    else:\n",
    "        base, span = created_at, (t_ready - created_at)\n",
    "    frac = np.random.beta(driver_cfg[\"alpha\"], driver_cfg[\"beta\"])\n",
    "    t_arr = base + span * frac\n",
    "    if t_arr >= t_pick:  # safety: keep strict order\n",
    "        t_arr = t_pick - MICRO(1)\n",
    "    return t_arr\n",
    "\n",
    "async def play_order(created_at, day, total):\n",
    "    oid=uuid.uuid4().hex; seq=0\n",
    "    _,lat,lon,addr = rand_customer(); items = rand_basket(day,total)\n",
    "    pts,dist = shortest_route(lat,lon); drive_min = dist/1609.34/GK_DRIVER_MPH*60\n",
    "\n",
    "    t_cs  = created_at + dt.timedelta(minutes=gauss(SVC[\"cs\"]))\n",
    "    t_sf  = t_cs + dt.timedelta(minutes=gauss(SVC[\"sf\"]))\n",
    "    t_fr  = t_sf + dt.timedelta(minutes=gauss(SVC[\"fr\"]))\n",
    "    t_ready = t_fr\n",
    "    t_pick  = t_ready + dt.timedelta(minutes=gauss(SVC[\"rp\"]))\n",
    "    t_drop  = t_pick  + dt.timedelta(minutes=drive_min)\n",
    "    t_arr   = driver_arrival_time(created_at, t_ready, t_pick)\n",
    "\n",
    "    enqueue(created_at+MICRO(seq),\"order_created\",oid,seq,\n",
    "            dict(customer_lat=lat,customer_lon=lon,customer_addr=addr,items=items)); seq+=1\n",
    "    enqueue(t_cs+MICRO(seq),\"gk_started\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_sf+MICRO(seq),\"gk_finished\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_ready+MICRO(seq),\"gk_ready\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_arr+MICRO(seq),\"driver_arrived\",oid,seq,{}); seq+=1\n",
    "    enqueue(t_pick+MICRO(seq),\"driver_picked_up\",oid,seq,\n",
    "            dict(route_points=pts, eta_mins=round(drive_min,1))); seq+=1\n",
    "\n",
    "    hops=max(1,int(drive_min*60//PING_SEC))\n",
    "    for h in range(1,hops):\n",
    "        p=h/hops; lat_i,lon_i=pts[int(p*(len(pts)-1))]\n",
    "        enqueue(t_pick+dt.timedelta(seconds=h*PING_SEC)+MICRO(seq),\"driver_ping\",oid,seq,\n",
    "                dict(progress_pct=round(p*100,1),loc_lat=lat_i,loc_lon=lon_i)); seq+=1\n",
    "\n",
    "    enqueue(t_drop+MICRO(seq),\"delivered\",oid,seq,\n",
    "            dict(delivered_lat=lat,delivered_lon=lon))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7.  BACK-FILL (single write)                                           â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_rows(ts, lat, lon, addr, items, pts, dist, svc, day, total):\n",
    "    rows=[]; seq=0; oid=uuid.uuid4().hex\n",
    "    drive=dist/1609.34/GK_DRIVER_MPH*60\n",
    "    t_cs=ts+dt.timedelta(minutes=gauss(svc[\"cs\"]))\n",
    "    t_sf=t_cs+dt.timedelta(minutes=gauss(svc[\"sf\"]))\n",
    "    t_fr=t_sf+dt.timedelta(minutes=gauss(svc[\"fr\"]))\n",
    "    t_ready=t_fr\n",
    "    t_pick=t_ready+dt.timedelta(minutes=gauss(svc[\"rp\"]))\n",
    "    t_drop=t_pick+dt.timedelta(minutes=drive)\n",
    "    t_arr=driver_arrival_time(ts, t_ready, t_pick)\n",
    "\n",
    "    def add(t,ev,p):\n",
    "        nonlocal seq\n",
    "        rows.append({\n",
    "            \"event_id\":uuid.uuid4().hex,\"event_type\":ev,\n",
    "            \"ts\":t.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\"gk_id\":GK_ID,\n",
    "            \"order_id\":oid,\"sequence\":seq,\n",
    "            \"body\":json.dumps(maybe_corrupt(ev,p))\n",
    "        }); seq+=1\n",
    "\n",
    "    add(ts,\"order_created\",dict(customer_lat=lat,customer_lon=lon,customer_addr=addr,items=items))\n",
    "    add(t_cs,\"gk_started\",{}); add(t_sf,\"gk_finished\",{}); add(t_ready,\"gk_ready\",{})\n",
    "    add(t_arr,\"driver_arrived\",{}); add(t_pick,\"driver_picked_up\",dict(route_points=pts,eta_mins=round(drive,1)))\n",
    "    hops=max(1,int(drive*60//PING_SEC))\n",
    "    for h in range(1,hops):\n",
    "        p=h/hops; lat_i,lon_i=pts[int(p*(len(pts)-1))]\n",
    "        add(t_pick+dt.timedelta(seconds=h*PING_SEC),\"driver_ping\",\n",
    "            dict(progress_pct=round(p*100,1),loc_lat=lat_i,loc_lon=lon_i))\n",
    "    add(t_drop,\"delivered\",dict(delivered_lat=lat,delivered_lon=lon))\n",
    "    return rows\n",
    "\n",
    "def gen_backfill(start, now, total_days):\n",
    "    back=[]\n",
    "    for d in range(total_days+1):\n",
    "        date = start.date()+dt.timedelta(days=d)\n",
    "        mean = orders_today(d,total_days,date)\n",
    "        lam  = mean/MIN_W.sum()*MIN_W; midnight=dt.datetime.combine(date,dt.time.min)\n",
    "        for m,v in enumerate(lam):\n",
    "            for _ in range(np.random.poisson(v)):\n",
    "                ts=midnight+dt.timedelta(minutes=m,seconds=RAND.randint(0,59))\n",
    "                if ts>=now: continue\n",
    "                _,lat,lon,addr = rand_customer(); items=rand_basket(d,total_days)\n",
    "                pts,dist=shortest_route(lat,lon)\n",
    "                back.extend(build_rows(ts,lat,lon,addr,items,pts,dist,SVC,d,total_days))\n",
    "    return back\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8.  MAIN SCHEDULER                                                     â”‚\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def schedule():\n",
    "    now = dt.datetime.utcnow()\n",
    "    total_days = (END_TS.date()-START_TS.date()).days\n",
    "\n",
    "    flush(gen_backfill(START_TS, now, total_days))   # back-fill\n",
    "    asyncio.create_task(consumer())                  # live consumer\n",
    "\n",
    "    async def later(ts, d):\n",
    "        await asyncio.sleep(max(0,(ts-now).total_seconds()/SPEED_UP))\n",
    "        await play_order(ts, d, total_days)\n",
    "\n",
    "    futures=[]\n",
    "    for d in range(total_days+1):\n",
    "        date = START_TS.date()+dt.timedelta(days=d)\n",
    "        mean = orders_today(d,total_days,date)\n",
    "        lam  = mean/MIN_W.sum()*MIN_W; midnight = dt.datetime.combine(date,dt.time.min)\n",
    "        for m,v in enumerate(lam):\n",
    "            for _ in range(np.random.poisson(v)):\n",
    "                ts=midnight+dt.timedelta(minutes=m,seconds=RAND.randint(0,59))\n",
    "                if ts>=now:\n",
    "                    futures.append(asyncio.create_task(later(ts,d)))\n",
    "    if futures: await asyncio.gather(*futures)\n",
    "\n",
    "print(f\"ðŸ‘»  GK-sim â†’ {TABLE_FQN}  (Ã—{SPEED_UP})\")\n",
    "print(f\"Running with config: {CFG}\")\n",
    "await schedule()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "simulator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
