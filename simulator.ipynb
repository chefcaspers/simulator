{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4294cd-d417-4151-8b71-d2b8875674a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "widgets"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "dbutils.widgets.text(\"catalog\", \"\") # gk_los_angeles\n",
    "dbutils.widgets.text(\"schema\", \"\") # default\n",
    "dbutils.widgets.text(\"table\", \"\") # events\n",
    "dbutils.widgets.text(\"start_ts\", \"\") # 2025-04-21 00:00:00\n",
    "dbutils.widgets.text(\"end_ts\", \"\") # 2025-05-31 00:00:00\n",
    "dbutils.widgets.text(\"start_orders\",\"\") # 100\n",
    "dbutils.widgets.text(\"end_orders\",\"\") # 200\n",
    "dbutils.widgets.text(\"metastore_root\",\"\") # only if you need to create the catalog with this notebook (see cell 3)\n",
    "dbutils.widgets.text(\"gk_location\", \"\") # 115 Penn St, El Segundo, CA 90245\n",
    "dbutils.widgets.text(\"speed_up_factor\", \"60\") # 1 means real time, 60 means 60 simulated minutes in 1 real minute, make this high to get fast flowing data, note that notebook will terminate when `end_ts` is hit (does not run indefinitely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea2193d-2881-447b-bfe2-52051227d2cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "data bootstrap"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ghost-kitchen menu generator\n",
    "â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "â€¢ Creates four relational datasets: brands, menus, categories, items\n",
    "â€¢ Adds deterministic, â€œreasonableâ€ USD prices to each item\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1. SOURCE DATA (renamed real-world menus) ----------\n",
    "brands_data = [\n",
    "    (\"McDoodles\", {\n",
    "        \"Burgers\": [\"Big Stack\", \"Quarter Poundish\", \"Double Quarter Poundish\", \"Cheese Burgerette\"],\n",
    "        \"Chicken & Fish Sandwiches\": [\"McCrisper\", \"Spicy McCrisper\", \"Filet-O-Fishish\", \"McChicklet\"],\n",
    "        \"Nuggets\": [\"Chick Nuggies 6pc\"],\n",
    "        \"Fries & Sides\": [\"Famous Frites\", \"Apple Slice Bites\"],\n",
    "        \"Breakfast\": [\"Egg McMuffle\", \"Bacon Biscuit\"],\n",
    "        \"Coffees & Beverages\": [\"Iced Brew\", \"Fizzy Cola\"]\n",
    "    }),\n",
    "    (\"ChipoLot\", {\n",
    "        \"Burritos\": [\"Chicken Burrito\", \"Steak Burrito\", \"Barbacoa Burrito\", \"Veggie Burrito\"],\n",
    "        \"Bowls\": [\"Chicken Bowl\", \"Keto Bowl\", \"High Protein Bowl\"],\n",
    "        \"Tacos\": [\"Chicken Tacos\", \"Carnitas Tacos\"],\n",
    "        \"Salads\": [\"Supergreens Salad\"],\n",
    "        \"Kids\": [\"Build-Your-Own Kid Meal\"],\n",
    "        \"Sides & Extras\": [\"Chips & Guac\", \"Chips & Queso\"]\n",
    "    }),\n",
    "    (\"Shack Stack\", {\n",
    "        \"Burgers\": [\"ShackBurger Single\", \"ShackBurger Double\", \"Smoke Stack Single\", \"Veggie Stack\"],\n",
    "        \"Chicken\": [\"Chicken Shack\", \"Avocado Bacon Chicken\"],\n",
    "        \"Dogs\": [\"Flat-Top Dog\"],\n",
    "        \"Fries\": [\"Crinkle Fries\", \"Cheese Fries\"],\n",
    "        \"Shakes\": [\"Vanilla Shake\", \"Chocolate Shake\"],\n",
    "        \"Drinks\": [\"Shack-made Lemonade\"]\n",
    "    }),\n",
    "    (\"GreenSprout\", {\n",
    "        \"Bowls\": [\"Harvest Bowl\", \"Crispy Rice Bowl\", \"Shroomami\"],\n",
    "        \"Salads\": [\"Kale Caesar\", \"Guac Greens\", \"Buffalo Chicken Salad\"],\n",
    "        \"Protein Plates\": [\"Garlic Steak Plate\", \"Hot Honey Chicken Plate\"],\n",
    "        \"Kids\": [\"Little Harvest\"],\n",
    "        \"Sides\": [\"Rosemary Focaccia\", \"Hummus & Focaccia\"],\n",
    "        \"Drinks\": [\"Jasmine Green Tea\"]\n",
    "    }),\n",
    "    (\"Pando Dash\", {\n",
    "        \"Entrees\": [\"Orange Chickadee\", \"Kung Pow Chicken\", \"Beijing Beefy\", \"Honey Walnut Shrimpish\"],\n",
    "        \"Sides\": [\"Chow Mein Noodles\", \"Fried Rice\"],\n",
    "        \"Appetizers\": [\"Cream Cheese Rangoon\", \"Chicken Egg Roll\"],\n",
    "        \"Drinks\": [\"Fountain Soda\"],\n",
    "        \"Bundles\": [\"Bowl Combo\"]\n",
    "    }),\n",
    "    (\"Kava Kitchen\", {\n",
    "        \"Bowls\": [\"Spicy Lamb & Avo Bowl\", \"Falafel Crunch Bowl\", \"Chicken + Rice Bowl\"],\n",
    "        \"Pitas\": [\"Steak & Feta Pita\", \"Crispy Falafel Pita\"],\n",
    "        \"Kids\": [\"Mini Pita Meal\"],\n",
    "        \"Drinks\": [\"Strawberry Citrus Lemonade\"],\n",
    "        \"Sides\": [\"Hot Harissa Chips\"],\n",
    "        \"Desserts\": [\"Salted Choco Cookie\"]\n",
    "    }),\n",
    "    (\"MaruNoodle\", {\n",
    "        \"Udon\": [\"Beef Udon\", \"Chicken Katsu Udon\", \"Curry Udon\"],\n",
    "        \"Rice Bowls\": [\"Gyudon Rice\", \"Yakitori Rice\"],\n",
    "        \"Tempura\": [\"Shrimp Tempura\", \"Sweet Potato Tempura\"],\n",
    "        \"Onigiri\": [\"Spam Musubi Onigiri\"],\n",
    "        \"Add-Ons\": [\"BK Sauce\"]\n",
    "    }),\n",
    "    (\"Jinya Noodle Bar\", {\n",
    "        \"Ramen\": [\"Tonkotsu Ramen\", \"Spicy Chicken Ramen\", \"Vegan Ramen\"],\n",
    "        \"Small Plates\": [\"Pork Gyoza\", \"Takoyaki\"],\n",
    "        \"Rice Bowls\": [\"Chicken Karaage Rice Bowl\"],\n",
    "        \"Toppings\": [\"Extra Chashu\", \"Spicy Sauce\"],\n",
    "        \"Drinks\": [\"Green Tea\"]\n",
    "    }),\n",
    "    (\"Fo Ho\", {\n",
    "        \"Pho\": [\"Pho Combo\", \"Pho Chicken\"],\n",
    "        \"Rolls\": [\"Fried Spring Roll\", \"Summer Roll\"],\n",
    "        \"Rice Plates\": [\"Grilled Pork Rice Plate\"],\n",
    "        \"Vermicelli\": [\"Lemongrass Chicken Vermicelli\"],\n",
    "        \"Drinks\": [\"Vietnamese Iced Coffee\"]\n",
    "    }),\n",
    "    (\"Yo! Sushii\", {\n",
    "        \"Maki\": [\"California Roll\", \"Spicy Tuna Roll\"],\n",
    "        \"Nigiri\": [\"Salmon Nigiri\", \"Ebi Nigiri\"],\n",
    "        \"Sashimi\": [\"Tuna Sashimi\"],\n",
    "        \"Street Food\": [\"Chicken Katsu Curry\", \"Yakisoba\", \"Miso Soup\"],\n",
    "        \"Desserts\": [\"Mochi Ice Cream\"],\n",
    "        \"Drinks\": [\"Matcha Soda\"]\n",
    "    }),\n",
    "    (\"Taco Ring\", {\n",
    "        \"Tacos\": [\"Crunchy Taco\", \"Soft Taco Supreme\"],\n",
    "        \"Burritos\": [\"Beefy 5-Layer Burrito\"],\n",
    "        \"Specialties\": [\"Doritos Locos Taco\"],\n",
    "        \"Quesadillas\": [\"Chicken Quesadilla\"],\n",
    "        \"Nachos\": [\"Nacho Bellgrande\"],\n",
    "        \"Sides\": [\"Cinnamon Twists\"],\n",
    "        \"Drinks\": [\"Baja Blast Soda\"]\n",
    "    }),\n",
    "    (\"Starbrews\", {\n",
    "        \"Hot Coffee\": [\"CaffÃ¨ Latte\", \"Pike Place Brew\"],\n",
    "        \"Cold Coffee\": [\"Cold Brew\"],\n",
    "        \"Refreshers\": [\"Strawberry AÃ§aÃ­ Refresher\"],\n",
    "        \"Frappuccinos\": [\"Caramel Frappuccino\"],\n",
    "        \"Bakery\": [\"Plain Bagel\"],\n",
    "        \"Breakfast Sandwiches\": [\"Sausage Cheddar Egg Sandwich\"],\n",
    "        \"Tea\": [\"Green Tea\"]\n",
    "    }),\n",
    "    (\"Dominni Pizza\", {\n",
    "        \"Specialty Pizzas\": [\"ExtravaganZZza Pizza\", \"MeatZZa Pizza\", \"Pacific Veggie Pizza\"],\n",
    "        \"Build Your Own Pizza\": [\"Custom Pizza\"],\n",
    "        \"Pasta\": [\"Chicken Alfredo Pasta\"],\n",
    "        \"Chicken\": [\"Boneless Chicken\"],\n",
    "        \"Sides\": [\"Bread Twists\"],\n",
    "        \"Desserts\": [\"Chocolate Lava Cake\"],\n",
    "        \"Drinks\": [\"Coke\"]\n",
    "    }),\n",
    "    (\"Five Gals\", {\n",
    "        \"Burgers\": [\"Bacon Cheeseburger\", \"Little Cheeseburger\"],\n",
    "        \"Dogs\": [\"Kosher Style Hot Dog\"],\n",
    "        \"Sandwiches\": [\"BLT Sandwich\"],\n",
    "        \"Fries\": [\"Cajun Fries\"],\n",
    "        \"Milkshakes\": [\"Vanilla Milkshake\"],\n",
    "        \"Drinks\": [\"Bottled Soda\"]\n",
    "    }),\n",
    "    (\"PokeCraft\", {\n",
    "        \"Poke Bowls\": [\"Signature Hawaiian Bowl\", \"Spicy Ahi Bowl\"],\n",
    "        \"Burritos\": [\"Chicken Katsu Burrito\"],\n",
    "        \"Salads\": [\"Vegan Salad\"],\n",
    "        \"Sides\": [\"Miso Soup Side\"],\n",
    "        \"Drinks\": [\"Japanese Soda\"]\n",
    "    })\n",
    "]\n",
    "\n",
    "# ---------- 2. PRICE BANDS BY CATEGORY KEYWORD ----------\n",
    "pricing_rules = [\n",
    "    ([\"pizza\"], (12, 18)),\n",
    "    ([\"burger\"], (6, 11)),\n",
    "    ([\"burrito\"], (7, 10)),\n",
    "    ([\"bowl\"], (9, 12)),\n",
    "    ([\"udon\", \"ramen\"], (10, 14)),\n",
    "    ([\"salad\"], (8, 11)),\n",
    "    ([\"pho\", \"rice plate\", \"plate\"], (9, 13)),\n",
    "    ([\"kids\"], (4, 6)),\n",
    "    ([\"fries\"], (2, 4)),\n",
    "    ([\"side\", \"sides\"], (2, 5)),\n",
    "    ([\"drink\", \"coffee\", \"tea\", \"soda\", \"lemonade\"], (2, 4)),\n",
    "    ([\"shake\", \"frappuccino\"], (4, 6)),\n",
    "    ([\"dessert\", \"mochi\", \"cookie\", \"cake\"], (3, 5)),\n",
    "    ([\"sandwich\"], (4, 7)),\n",
    "    ([\"dog\"], (3, 6)),\n",
    "    ([\"nugget\"], (3, 5)),\n",
    "    ([\"taco\"], (2, 4)),\n",
    "    ([\"quesa\", \"nacho\"], (3, 6)),\n",
    "    ([\"roll\", \"nigiri\"], (4, 7)),\n",
    "    ([\"sashimi\"], (5, 9)),\n",
    "    ([\"tempura\"], (4, 7)),\n",
    "    ([\"onigiri\"], (3, 5)),\n",
    "    ([\"rice\"], (8, 11)),\n",
    "]\n",
    "\n",
    "def deterministic_random(item_name: str, low: int, high: int) -> float:\n",
    "    \"\"\"Hash item name to a deterministic price within [low, high).\"\"\"\n",
    "    h = hashlib.md5(item_name.encode()).hexdigest()\n",
    "    cents_range = (high - low) * 100\n",
    "    return round(low + (int(h[:8], 16) % cents_range) / 100, 2)\n",
    "\n",
    "def get_price(item_name: str, category_name: str) -> float:\n",
    "    lower_cat = category_name.lower()\n",
    "    for keywords, (lo, hi) in pricing_rules:\n",
    "        if any(k in lower_cat for k in keywords):\n",
    "            return deterministic_random(item_name, lo, hi)\n",
    "    return deterministic_random(item_name, 6, 10)  # fallback\n",
    "\n",
    "# ---------- 3. BUILD TABLE ROWS ----------\n",
    "brands_rows, menus_rows, categories_rows, items_rows = [], [], [], []\n",
    "\n",
    "brand_id = menu_id = category_id = item_id = 1\n",
    "for brand, menu_struct in brands_data:\n",
    "    brands_rows.append({\"id\": brand_id, \"name\": brand})\n",
    "    menus_rows.append({\"id\": menu_id, \"brand_id\": brand_id, \"name\": f\"{brand} Main Menu\"})\n",
    "    for cat, items in menu_struct.items():\n",
    "        categories_rows.append({\"id\": category_id, \"menu_id\": menu_id, \"brand_id\": brand_id,\"name\": cat})\n",
    "        for it in items:\n",
    "            price = get_price(it, cat)\n",
    "            items_rows.append({\n",
    "                \"id\": item_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"menu_id\": menu_id,\n",
    "                \"brand_id\": brand_id,\n",
    "                \"name\": it,\n",
    "                \"price\": price\n",
    "            })\n",
    "            item_id += 1\n",
    "        category_id += 1\n",
    "    brand_id += 1\n",
    "    menu_id += 1\n",
    "\n",
    "# ---------- 4. TO DATAFRAMES ----------\n",
    "brands_df     = pd.DataFrame(brands_rows)\n",
    "menus_df      = pd.DataFrame(menus_rows)\n",
    "categories_df = pd.DataFrame(categories_rows)\n",
    "items_df      = pd.DataFrame(items_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb10d0ea-b244-4b3b-b109-650bcf4f6982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS ${catalog} MANAGED LOCATION '${metastore_root}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c19b9f7-e685-4913-91a5-7cb9c7b00ed5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "catalog entires"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG ${catalog};\n",
    "DROP SCHEMA ${schema} CASCADE;\n",
    "CREATE SCHEMA ${schema};\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ${catalog}.${schema}.brand (\n",
    "  id   BIGINT NOT NULL,\n",
    "  name STRING NOT NULL,\n",
    "  CONSTRAINT brand_pk PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ${catalog}.${schema}.menu (\n",
    "  id       BIGINT NOT NULL,\n",
    "  name     STRING NOT NULL,\n",
    "  brand_id BIGINT NOT NULL,\n",
    "\n",
    "  CONSTRAINT menu_pk        PRIMARY KEY (id),\n",
    "  CONSTRAINT menu_brand_fk  FOREIGN KEY (brand_id)\n",
    "    REFERENCES ${catalog}.${schema}.brand(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ${catalog}.${schema}.category (\n",
    "  id       BIGINT NOT NULL,\n",
    "  name     STRING NOT NULL,\n",
    "  menu_id  BIGINT NOT NULL,\n",
    "  brand_id BIGINT NOT NULL,\n",
    "\n",
    "  CONSTRAINT category_pk         PRIMARY KEY (id),\n",
    "  CONSTRAINT category_menu_fk    FOREIGN KEY (menu_id)\n",
    "      REFERENCES ${catalog}.${schema}.menu(id),\n",
    "  CONSTRAINT category_brand_fk   FOREIGN KEY (brand_id)\n",
    "      REFERENCES ${catalog}.${schema}.brand(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ${catalog}.${schema}.item (\n",
    "  id          BIGINT NOT NULL,\n",
    "  name        STRING NOT NULL,\n",
    "  description STRING,\n",
    "  price       DOUBLE NOT NULL,\n",
    "  image_data  STRING,\n",
    "\n",
    "  brand_id    BIGINT NOT NULL,\n",
    "  menu_id     BIGINT NOT NULL,\n",
    "  category_id BIGINT NOT NULL,\n",
    "\n",
    "  CONSTRAINT item_pk            PRIMARY KEY (id),\n",
    "  CONSTRAINT item_brand_fk      FOREIGN KEY (brand_id)\n",
    "      REFERENCES ${catalog}.${schema}.brand(id),\n",
    "  CONSTRAINT item_menu_fk       FOREIGN KEY (menu_id)\n",
    "      REFERENCES ${catalog}.${schema}.menu(id),\n",
    "  CONSTRAINT item_category_fk   FOREIGN KEY (category_id)\n",
    "      REFERENCES ${catalog}.${schema}.category(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ${catalog}.${schema}.${table} (\n",
    "  event_id   STRING,\n",
    "  event_type STRING,\n",
    "  ts         TIMESTAMP,\n",
    "  gk_id      STRING,\n",
    "  order_id   STRING,\n",
    "  sequence   INT,\n",
    "  body       STRING,\n",
    "  day        DATE\n",
    ")\n",
    "USING delta\n",
    "PARTITIONED BY (day)\n",
    "TBLPROPERTIES(delta.minReaderVersion=2, delta.minWriterVersion=5);\n",
    "\n",
    "CREATE VOLUME ${catalog}.${schema}.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eeee9de-0b1e-47e0-b941-d4c054f1c95f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write data to catalog"
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(brands_df).write.mode(\"overwrite\").saveAsTable(\"brand\")\n",
    "spark.createDataFrame(menus_df).write.mode(\"overwrite\").saveAsTable(\"menu\")\n",
    "spark.createDataFrame(categories_df).write.mode(\"overwrite\").saveAsTable(\"category\")\n",
    "spark.createDataFrame(items_df).write.mode(\"overwrite\").saveAsTable(\"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6fb700-d026-4b77-9997-b8a7560b0a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install osmnx==1.7.1 networkx==3.2.1 geopy==2.4.1 shapely==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000b80e0-b045-41b1-b55b-6f0b6d45501a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Ghost-Kitchen Event Simulator â†’ Delta table                              â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#\n",
    "#  â€£ Uses osmnx + networkx for routing\n",
    "#  â€£ Writes one batch for back-fill, then streams live events in order\n",
    "#\n",
    "#  First-time dependency install (once per cluster):\n",
    "#  %pip install osmnx==1.7.1 networkx==3.2.1 geopy==2.4.1 nest_asyncio==1.6.0\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import asyncio\n",
    "import datetime as dt\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nest_asyncio\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "nest_asyncio.apply()                    # allow nested event loops\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  1.  READ WIDGETS / CONSTANTS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CATALOG  : str = dbutils.widgets.get(\"catalog\")      # e.g. \"main\"\n",
    "SCHEMA   : str = dbutils.widgets.get(\"schema\")       # e.g. \"gk_sim\"\n",
    "TABLE    : str = dbutils.widgets.get(\"table\")        # e.g. \"events\"\n",
    "TABLE_FQN          = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "\n",
    "START_TS           = dbutils.widgets.get(\"start_ts\") # \"2025-05-12 00:00:00\"\n",
    "END_TS             = dbutils.widgets.get(\"end_ts\")   # \"2025-05-14 00:00:00\"\n",
    "START_ORDERS       = int(dbutils.widgets.get(\"start_orders\"))\n",
    "END_ORDERS         = int(dbutils.widgets.get(\"end_orders\"))\n",
    "\n",
    "GK_ADDRESS         = dbutils.widgets.get(\"gk_location\")\n",
    "GK_RADIUS_MI       = 2.0\n",
    "GK_DRIVER_MPH      = 25.0\n",
    "\n",
    "CACHE_DIR          = f\"/Volumes/{CATALOG}/{SCHEMA}/cache\"\n",
    "RANDOM_SEED        = 42\n",
    "SPEED_UP           = int(dbutils.widgets.get(\"speed_up_factor\"))                 # 1 sim-minute = 1 real-second\n",
    "PING_INTERVAL_SEC  = 60\n",
    "BATCH_ROWS         = 200                # flush threshold\n",
    "BATCH_SECONDS      = 1.0\n",
    "\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  2.  SIMULATION PARAMETERS THAT REMAIN VARIABLE DAY-TO-DAY\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "C = {\n",
    "    # day-to-day volume noise (Â±10 % by default)\n",
    "    \"noise\": 0.10,\n",
    "\n",
    "    # service-stage mean/deviation in minutes:\n",
    "    #   createdâ†’started, startedâ†’finished, finishedâ†’ready, readyâ†’pickup\n",
    "    \"svc\": {\n",
    "        \"cs\": (2, 1),     # chef start\n",
    "        \"sf\": (10, 3),    # cooking\n",
    "        \"fr\": (2, 1),     # boxing\n",
    "        \"rp\": (6, 2)      # wait for driver\n",
    "    }\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  3.  BUILD / LOAD ROAD GRAPH + CUSTOMER NODE CANDIDATES\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CACHE_PATH = Path(CACHE_DIR); CACHE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "GRAPH_PKL  = CACHE_PATH / \"roadGraph.pkl\"\n",
    "NODES_PARQ = CACHE_PATH / \"nodes.parquet\"\n",
    "\n",
    "def load_road_graph() -> nx.MultiDiGraph:\n",
    "    \"\"\"Load from cache or download a drivable road graph around the kitchen.\"\"\"\n",
    "    if GRAPH_PKL.exists():\n",
    "        return pickle.loads(GRAPH_PKL.read_bytes())\n",
    "    ox.settings.log_console = False\n",
    "    graph = ox.graph_from_point(\n",
    "        ox.geocoder.geocode(GK_ADDRESS),\n",
    "        dist=GK_RADIUS_MI * 1609.34,\n",
    "        network_type=\"drive\"\n",
    "    )\n",
    "    GRAPH_PKL.write_bytes(pickle.dumps(graph))\n",
    "    return graph\n",
    "\n",
    "def load_candidate_nodes(graph: nx.MultiDiGraph) -> pd.DataFrame:\n",
    "    \"\"\"Return Geo-qualified nodes in the same connected component as the kitchen.\"\"\"\n",
    "    if NODES_PARQ.exists():\n",
    "        return pd.read_parquet(NODES_PARQ)\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for node_id, data in graph.nodes(data=True):\n",
    "        house, street = data.get(\"addr:housenumber\"), data.get(\"addr:street\")\n",
    "        label = f\"{house} {street}\" if house and street else f\"{random.randint(1,9999)} Main St\"\n",
    "        rows.append(\n",
    "            {\"node_id\": node_id, \"lat\": data[\"y\"], \"lon\": data[\"x\"], \"addr\": label}\n",
    "        )\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_parquet(NODES_PARQ, index=False)\n",
    "    return df\n",
    "\n",
    "GRAPH = load_road_graph()\n",
    "CANDIDATE_NODES = load_candidate_nodes(GRAPH)\n",
    "\n",
    "GK_LAT, GK_LON = ox.geocoder.geocode(GK_ADDRESS)\n",
    "GK_NODE = ox.distance.nearest_nodes(GRAPH, GK_LON, GK_LAT)\n",
    "\n",
    "# keep nodes in same undirected component as the kitchen\n",
    "component_id = {n: cid for cid, comp in enumerate(nx.connected_components(GRAPH.to_undirected())) for n in comp}\n",
    "CANDIDATE_NODES = CANDIDATE_NODES[CANDIDATE_NODES[\"node_id\"].map(component_id) == component_id[GK_NODE]].reset_index(drop=True)\n",
    "\n",
    "def random_customer() -> Tuple[int, float, float, str]:\n",
    "    \"\"\"Return random reachable customer node (id, lat, lon, addr).\"\"\"\n",
    "    row = CANDIDATE_NODES.sample(1).iloc[0]\n",
    "    return int(row.node_id), row.lat, row.lon, row.addr\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  4.  MENU SNAPSHOT (simplified)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ITEMS_DF = spark.table(\"item\").toPandas()        # expects pre-populated table\n",
    "ITEMS_BY_BRAND = {bid: grp.to_dict(\"records\") for bid, grp in ITEMS_DF.groupby(\"brand_id\")}\n",
    "BRAND_IDS = list(ITEMS_BY_BRAND)\n",
    "\n",
    "def random_basket(single_brand_share: float = 0.7,\n",
    "                  max_brands: int = 4,\n",
    "                  items_range: Tuple[int,int] = (1,4)) -> List[Dict]:\n",
    "    \"\"\"Return a list of item dicts with qty.\"\"\"\n",
    "    if random.random() < single_brand_share:\n",
    "        brands = [random.choice(BRAND_IDS)]\n",
    "    else:\n",
    "        brands = random.sample(BRAND_IDS, random.randint(2, max_brands))\n",
    "\n",
    "    items: List[Dict] = []\n",
    "    for brand in brands:\n",
    "        for item in random.sample(ITEMS_BY_BRAND[brand], random.randint(*items_range)):\n",
    "            record = item.copy()\n",
    "            record[\"qty\"] = random.randint(1, 3)\n",
    "            items.append(record)\n",
    "    return items\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  5.  VOLUME MODELS (daily trend + minute curve)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def minute_weight_vector() -> np.ndarray:\n",
    "    \"\"\"Sinusoidal lift for lunch/dinner windows (length 1440).\"\"\"\n",
    "    weights = np.ones(1440)\n",
    "    for peak in (\n",
    "        (\"11:00\", \"13:30\", 3.0),     # lunch\n",
    "        (\"17:00\", \"20:00\", 3.5)      # dinner\n",
    "    ):\n",
    "        start, end, mult = peak\n",
    "        s = dt.datetime.strptime(start, \"%H:%M\")\n",
    "        e = dt.datetime.strptime(end,   \"%H:%M\")\n",
    "        span = e.hour*60+e.minute - (s.hour*60+s.minute)\n",
    "        for m in range(s.hour*60+s.minute, e.hour*60+e.minute):\n",
    "            x = (m - (s.hour*60+s.minute)) / span\n",
    "            weights[m] += (mult-1) * math.sin(math.pi*x)**2\n",
    "    return weights\n",
    "\n",
    "MINUTE_WEIGHTS = minute_weight_vector()\n",
    "\n",
    "def orders_per_day(day_index: int, total_days: int) -> float:\n",
    "    \"\"\"Linear growth from START_ORDERS to END_ORDERS across full range.\"\"\"\n",
    "    return START_ORDERS + (END_ORDERS - START_ORDERS) * (day_index / total_days)\n",
    "\n",
    "def weekday_multiplier(date_obj: dt.date) -> float:\n",
    "    mapping = {\"mon\":1,\"tue\":1.05,\"wed\":1.08,\"thu\":1.10,\"fri\":1.25,\"sat\":1.35,\"sun\":1.15}\n",
    "    return mapping[date_obj.strftime(\"%a\").lower()]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  6.  ROUTING HELPERS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def shortest_route(lat: float, lon: float) -> Tuple[List[Tuple[float,float]], float]:\n",
    "    \"\"\"\n",
    "    Return (polyline list[(lat,lon)], distance_m).\n",
    "    Falls back to undirected graph if directed path not found.\n",
    "    \"\"\"\n",
    "    customer_node = ox.distance.nearest_nodes(GRAPH, lon, lat)\n",
    "\n",
    "    try:\n",
    "        path = nx.shortest_path(GRAPH, GK_NODE, customer_node, weight=\"length\")\n",
    "        graph_used = GRAPH\n",
    "    except nx.NetworkXNoPath:\n",
    "        graph_used = GRAPH.to_undirected()\n",
    "        path = nx.shortest_path(graph_used, GK_NODE, customer_node, weight=\"length\")\n",
    "\n",
    "    coords = [(graph_used.nodes[n][\"y\"], graph_used.nodes[n][\"x\"]) for n in path]\n",
    "\n",
    "    distance_m = 0.0\n",
    "    for u, v in zip(path[:-1], path[1:]):\n",
    "        try:\n",
    "            lengths = [d[\"length\"] for d in graph_used[u][v].values()]\n",
    "        except KeyError:\n",
    "            lengths = [d[\"length\"] for d in graph_used[v][u].values()]\n",
    "        distance_m += min(lengths)\n",
    "\n",
    "    return coords, distance_m\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  7.  DELTA WRITER UTILITIES\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EVENT_QUEUE : asyncio.PriorityQueue = asyncio.PriorityQueue()\n",
    "COUNTER = 0                          # unique tiebreaker\n",
    "GHOST_KITCHEN_ID = uuid.uuid4().hex\n",
    "\n",
    "def enqueue_event(ts: dt.datetime,\n",
    "                  event_type: str,\n",
    "                  order_id: str,\n",
    "                  sequence: int,\n",
    "                  payload: Dict):\n",
    "    \"\"\"\n",
    "    Put event into priority queue ordered by (epoch, COUNTER).\n",
    "    The queueâ€™s consumer flushes to Delta.\n",
    "    \"\"\"\n",
    "    global COUNTER\n",
    "    COUNTER += 1\n",
    "    EVENT_QUEUE.put_nowait((\n",
    "        ts.timestamp(),\n",
    "        COUNTER,\n",
    "        {\n",
    "            \"event_id\": uuid.uuid4().hex,\n",
    "            \"event_type\": event_type,\n",
    "            \"ts\": ts.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
    "            \"gk_id\": GHOST_KITCHEN_ID,\n",
    "            \"order_id\": order_id,\n",
    "            \"sequence\": sequence,\n",
    "            \"body\": json.dumps(payload)\n",
    "        }\n",
    "    ))\n",
    "\n",
    "def flush_batch(rows: List[Dict]):\n",
    "    \"\"\"Write a batch of dict rows to Delta.\"\"\"\n",
    "    if not rows:\n",
    "        return\n",
    "    (\n",
    "      spark.createDataFrame(rows)\n",
    "           .withColumn(\"ts\",  F.to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm:ss.SSSSSS\"))\n",
    "           .withColumn(\"day\", F.to_date(\"ts\"))\n",
    "           .withColumn(\"sequence\", F.col(\"sequence\").cast(\"INT\"))\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"append\")\n",
    "           .saveAsTable(TABLE_FQN)\n",
    "    )\n",
    "\n",
    "async def queue_consumer():\n",
    "    \"\"\"Continuously flush the priority queue to Delta in small batches.\"\"\"\n",
    "    buffer, last_flush = [], time.time()\n",
    "    while True:\n",
    "        _, _, row = await EVENT_QUEUE.get()\n",
    "        buffer.append(row)\n",
    "        if len(buffer) >= BATCH_ROWS or (time.time() - last_flush) >= BATCH_SECONDS:\n",
    "            flush_batch(buffer)\n",
    "            buffer.clear(); last_flush = time.time()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  8.  ORDER LIFECYCLE COROUTINE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MICRO = lambda n: dt.timedelta(microseconds=n)   # avoid time collisions with this\n",
    "NORM  = lambda mean_sd: max(0.1, random.gauss(*mean_sd))\n",
    "\n",
    "async def play_order(created_at: dt.datetime):\n",
    "    \"\"\"Generate + enqueue all events for a single order.\"\"\"\n",
    "    order_id = uuid.uuid4().hex\n",
    "    seq_num  = 0\n",
    "\n",
    "    node_id, lat, lon, addr = random_customer()\n",
    "    items = random_basket()\n",
    "\n",
    "    route_points, dist_m = shortest_route(lat, lon)\n",
    "    drive_minutes = (dist_m / 1609.34) / GK_DRIVER_MPH * 60\n",
    "\n",
    "    svc = C[\"svc\"]\n",
    "    t_start   = created_at + dt.timedelta(minutes=NORM(svc[\"cs\"]))\n",
    "    t_finish  = t_start   + dt.timedelta(minutes=NORM(svc[\"sf\"]))\n",
    "    t_ready   = t_finish  + dt.timedelta(minutes=NORM(svc[\"fr\"]))\n",
    "    t_pickup  = t_ready   + dt.timedelta(minutes=NORM(svc[\"rp\"]))\n",
    "    t_deliver = t_pickup  + dt.timedelta(minutes=drive_minutes)\n",
    "\n",
    "    enqueue_event(created_at+MICRO(seq_num), \"order_created\", order_id, seq_num,\n",
    "                  {\"customer_lat\": lat, \"customer_lon\": lon, \"customer_addr\": addr, \"items\": items}); seq_num += 1\n",
    "    enqueue_event(t_start+MICRO(seq_num), \"gk_started\", order_id, seq_num, {}); seq_num += 1\n",
    "    enqueue_event(t_finish+MICRO(seq_num), \"gk_finished\", order_id, seq_num, {}); seq_num += 1\n",
    "    enqueue_event(t_ready+MICRO(seq_num), \"gk_ready\", order_id, seq_num, {}); seq_num += 1\n",
    "    enqueue_event(t_pickup+MICRO(seq_num), \"driver_picked_up\", order_id, seq_num,\n",
    "                  {\"route_points\": route_points, \"eta_mins\": round(drive_minutes, 1)}); seq_num += 1\n",
    "\n",
    "    # driver pings\n",
    "    hops = max(1, int(drive_minutes*60 // PING_INTERVAL_SEC))\n",
    "    for hop in range(1, hops):\n",
    "        progress = hop / hops\n",
    "        lat_h, lon_h = route_points[int(progress*(len(route_points)-1))]\n",
    "        enqueue_event(\n",
    "            t_pickup + dt.timedelta(seconds=hop*PING_INTERVAL_SEC) + MICRO(seq_num),\n",
    "            \"driver_ping\",\n",
    "            order_id,\n",
    "            seq_num,\n",
    "            {\"progress_pct\": round(progress*100, 1), \"loc_lat\": lat_h, \"loc_lon\": lon_h}\n",
    "        )\n",
    "        seq_num += 1\n",
    "\n",
    "    enqueue_event(t_deliver+MICRO(seq_num), \"delivered\", order_id, seq_num,\n",
    "                  {\"delivered_lat\": lat, \"delivered_lon\": lon})\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  9.  BACK-FILL BATCH  +  REAL-TIME SCHEDULER\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def run_scheduler():\n",
    "    \"\"\"Generate back-fill instantly, then stream future events in real time.\"\"\"\n",
    "    start_dt = dt.datetime.strptime(START_TS, \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_dt   = dt.datetime.strptime(END_TS,   \"%Y-%m-%d %H:%M:%S\")\n",
    "    now_dt   = dt.datetime.utcnow()\n",
    "\n",
    "    total_days = (end_dt.date() - start_dt.date()).days\n",
    "\n",
    "    async def schedule_future(ts: dt.datetime):\n",
    "        \"\"\"Sleep (accelerated) then play the order.\"\"\"\n",
    "        delay = (ts - now_dt).total_seconds() / SPEED_UP\n",
    "        await asyncio.sleep(max(0, delay))\n",
    "        await play_order(ts)\n",
    "\n",
    "    future_tasks: List[asyncio.Task] = []\n",
    "\n",
    "    for day_offset in range(total_days + 1):\n",
    "        current_date = start_dt.date() + dt.timedelta(days=day_offset)\n",
    "\n",
    "        # expected mean orders for this day\n",
    "        mean_orders = (orders_per_day(day_offset, total_days)\n",
    "                       * weekday_multiplier(current_date)\n",
    "                       * random.uniform(1-C[\"noise\"], 1+C[\"noise\"]))\n",
    "\n",
    "        minute_lambdas = mean_orders / MINUTE_WEIGHTS.sum() * MINUTE_WEIGHTS\n",
    "\n",
    "        midnight = dt.datetime.combine(current_date, dt.time.min)\n",
    "\n",
    "        for minute, lam in enumerate(minute_lambdas):\n",
    "            count = np.random.poisson(lam)\n",
    "            for _ in range(count):\n",
    "                timestamp = midnight + dt.timedelta(minutes=minute,\n",
    "                                                    seconds=random.randint(0,59))\n",
    "\n",
    "                if timestamp < now_dt:\n",
    "                    await play_order(timestamp)               # back-fill\n",
    "                else:\n",
    "                    future_tasks.append(asyncio.create_task(schedule_future(timestamp)))\n",
    "\n",
    "    # start the consumer only AFTER back-fill has filled the queue once\n",
    "    consumer_task = asyncio.create_task(queue_consumer())\n",
    "\n",
    "    if future_tasks:\n",
    "        await asyncio.gather(*future_tasks)\n",
    "    # allow remaining live events to flush\n",
    "    await asyncio.sleep(5/SPEED_UP)\n",
    "    consumer_task.cancel()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 10. MAIN ENTRY\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"ðŸ‘»  Ghost-Kitchen simulator â†’ {TABLE_FQN}  (speedÃ—{SPEED_UP})\")\n",
    "await run_scheduler()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4674553830868044,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "simulator",
   "widgets": {
    "catalog": {
     "currentValue": "gk_los_angeles",
     "nuid": "757198ad-8556-4a6d-b791-eaa2ef6d9dd6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "end_orders": {
     "currentValue": "150",
     "nuid": "66681e20-42ae-478b-b4fc-44eb9f16ab9f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "end_orders",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "end_orders",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "end_ts": {
     "currentValue": "2025-05-31 00:00:00",
     "nuid": "ec51b9c3-0ca4-4a98-ba04-de73d10cff95",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "end_ts",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "end_ts",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "gk_location": {
     "currentValue": "115 Penn St, El Segundo, CA 90245",
     "nuid": "a94d0f65-29d3-41f7-8817-6cbf19f68841",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "gk_location",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "gk_location",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "metastore_root": {
     "currentValue": "s3://databricks-5lnerubfrhsig9but2ah84-cloud-storage-bucket/unity-catalog/2947840311790182/",
     "nuid": "cfc7499c-1fbd-4dba-b3af-f1def1ff73c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "metastore_root",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "metastore_root",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "default",
     "nuid": "99e1b7e1-0665-4bc3-94e1-73bc52417086",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "speed_up_factor": {
     "currentValue": "60",
     "nuid": "32abc8a9-4525-4344-9e88-3cefba822312",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "speed_up_factor",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "speed_up_factor",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "start_orders": {
     "currentValue": "100",
     "nuid": "140cabbd-8400-41a2-91f1-bb8d052f8512",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "start_orders",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "start_orders",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_ts": {
     "currentValue": "2025-04-21 00:00:00",
     "nuid": "8d7666ab-52c8-4e5b-89df-f11c7527d97c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "start_ts",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "start_ts",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table": {
     "currentValue": "events",
     "nuid": "ad6f1879-5531-4b5f-91ba-94ce04fec002",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
